{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5LG-pqBFSpl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igVNpufMbUK2"
   },
   "outputs": [],
   "source": [
    "load_path = 'data/data_final.csv'\n",
    "original_df = pd.read_csv(load_path, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qgnx79wc_IUi"
   },
   "outputs": [],
   "source": [
    "def generate_entailment_prompt(tweet, claim, cot_true, shot_true, front_true=False):\n",
    "    if cot_true==True:\n",
    "        cot=\" Let's think step by step.\"\n",
    "    else:\n",
    "        cot=\"\"\n",
    "\n",
    "    if shot_true==True:\n",
    "        shot=\"\"\"\n",
    "TWEET: A dog is running in a field.\n",
    "CLAIM: An animal is running in a field.\n",
    "ANSWER: A dog is an animal. A dog running in a field is an animal running in a field. So the final answer is ENTAILMENT.\n",
    "\n",
    "TWEET: A man is breaking three eggs in a bowl.\n",
    "CLAIM: A girl is pouring some milk in a bowl.\n",
    "ANSWER: A man is breaking three eggs in a bowl does not imply that a girl is pouring some milk in a bowl. So the final answer is NEUTRAL.\n",
    "\n",
    "TWEET: A man is playing golf.\n",
    "CLAIM: No man is playing golf.\n",
    "ANSWER: A man is playing golf and no man is playing golf cannot be true at the same time. So the final answer is CONTRADICTION.\n",
    "\"\"\"\n",
    "    else:\n",
    "        shot=\"\"\n",
    "\n",
    "    if front_true==True:\n",
    "        constraint=\"You must first choose from ENTAILMENT, NEUTRAL, or CONTRADICTION, and then provide an explanation.\"\n",
    "\n",
    "    elif front_true==False:\n",
    "        constraint=\"You must provide an explanation, and then a final choice as ENTAILMENT, NEUTRAL, or CONTRADICTION.\"\n",
    "\n",
    "    else:\n",
    "        constraint=\"Your answer should only be either ENTAILMENT, NEUTRAL, or CONTRADICTION.\"\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>> Which of the following best describes the relationship between TWEET and CLAIM? {constraint}\n",
    "\n",
    "If TWEET is true:\n",
    "(ENTAILMENT) then CLAIM is also true.\n",
    "(NEUTRAL) CLAIM cannot be said to be true or false.\n",
    "(CONTRADICTION) then CLAIM is false. <</SYS>>\n",
    "{shot}\n",
    "TWEET: {tweet}\n",
    "CLAIM: {claim}\n",
    "ANSWER:{cot} [/INST]\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAAnIaCxj59o",
    "outputId": "3441024f-40ac-42a2-8216-299f14aefe8a"
   },
   "outputs": [],
   "source": [
    "list(original_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuimG4Ff5Rtv"
   },
   "source": [
    "# Create Balanced and Imbalanced Train Dataset, both in original and reversed order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6GNd89i5YcD"
   },
   "outputs": [],
   "source": [
    "def get_train_set(original_df, model_name, balanced, reversed=False):\n",
    "\n",
    "    if balanced==True:\n",
    "        b_suffix='_balanced'\n",
    "    elif balanced==False:\n",
    "        b_suffix='_unbalanced'\n",
    "\n",
    "    if reversed==True:\n",
    "        suffix='_reversed'\n",
    "    elif reversed==False:\n",
    "        suffix=''\n",
    "\n",
    "    training_list = []\n",
    "\n",
    "    for index, row in original_df.iterrows():\n",
    "\n",
    "        entailment = {'old_index': index,\n",
    "                      'claim_number': row['claim_number'],\n",
    "                      'claim': row['claim'],\n",
    "                      'generated_tweet': row[f'generated_entail_tweet{suffix}_{model_name}'],\n",
    "                      'ground_truth': 'ENTAILMENT'}\n",
    "\n",
    "        neutral = {'old_index': index,\n",
    "                    'claim_number': row['claim_number'],\n",
    "                    'claim': row['claim'],\n",
    "                    'generated_tweet': row[f'generated_neutral_tweet{suffix}_{model_name}'],\n",
    "                    'ground_truth': 'NEUTRAL'}\n",
    "\n",
    "        contradiction = {'old_index': index,\n",
    "            'claim_number': row['claim_number'],\n",
    "            'claim': row['claim'],\n",
    "            'generated_tweet': row[f'generated_contradict_tweet{suffix}_{model_name}'],\n",
    "            'ground_truth': 'CONTRADICTION'}\n",
    "\n",
    "        training_list.append(contradiction)\n",
    "        training_list.append(entailment)\n",
    "        training_list.append(neutral)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(training_list)\n",
    "\n",
    "    if balanced == False:\n",
    "        # Separate the classes\n",
    "        df_entailment = df[df.ground_truth == 'ENTAILMENT']\n",
    "        df_neutral = df[df.ground_truth == 'NEUTRAL']\n",
    "        df_contradiction = df[df.ground_truth == 'CONTRADICTION']\n",
    "\n",
    "        # Define new sample sizes\n",
    "        n_total = len(df)\n",
    "        n_entailment = int(0.5 * n_total)\n",
    "        n_neutral = int(0.35 * n_total)\n",
    "        n_contradiction = n_total - n_entailment - n_neutral\n",
    "\n",
    "        # Resample the DataFrames\n",
    "        df_entailment_resampled = resample(df_entailment, replace=True, n_samples=n_entailment, random_state=42)\n",
    "        df_neutral_resampled = resample(df_neutral, replace=True, n_samples=n_neutral, random_state=42)\n",
    "        df_contradiction_resampled = resample(df_contradiction, replace=False, n_samples=n_contradiction, random_state=42)\n",
    "\n",
    "        # Concatenate the resampled DataFrames\n",
    "        df = pd.concat([df_entailment_resampled, df_neutral_resampled, df_contradiction_resampled])\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_dataset = []\n",
    "    for i, row in df.iterrows():\n",
    "        tweet = row['generated_tweet']\n",
    "        claim = row['claim']\n",
    "        ground_truth = row['ground_truth']\n",
    "\n",
    "        if reversed==False:\n",
    "\n",
    "            datum = {\n",
    "\"instruction\": f\"\"\"<<SYS>> Which of the following best describes the relationship between TWEET and CLAIM?\n",
    "\n",
    "If TWEET is true:\n",
    "(ENTAILMENT) then CLAIM is also true.\n",
    "(NEUTRAL) CLAIM cannot be said to be true or false.\n",
    "(CONTRADICTION) then CLAIM is false. <</SYS>>\"\"\",\n",
    "\n",
    "\"input\": f\"\"\"TWEET: {tweet}\n",
    "CLAIM: {claim}\n",
    "ANSWER:\"\"\",\n",
    "\n",
    "\"output\": f\"{ground_truth}\"\n",
    "\n",
    "            }\n",
    "\n",
    "        elif reversed==True:\n",
    "\n",
    "            datum = {\n",
    "\n",
    "\"instruction\": f\"\"\"<<SYS> Which of the following best describes the relationship between CLAIM and TWEET?\n",
    "\n",
    "If CLAIM is true:\n",
    "(ENTAILMENT) then TWEET is also true.\n",
    "(NEUTRAL) TWEET cannot be said to be true or false.\n",
    "(CONTRADICTION) then TWEET is false. <</SYS>>\"\"\",\n",
    "\n",
    "\"input\": f\"\"\"CLAIM: {claim}\n",
    "TWEET: {tweet}\n",
    "ANSWER:\"\"\",\n",
    "\n",
    "\"output\": f\"{ground_truth}\"\n",
    "\n",
    "             }\n",
    "\n",
    "        train_dataset.append(datum)\n",
    "\n",
    "    with open(f'data/llama_train_json/train_{model_name}{b_suffix}{suffix}.json', 'w') as f:\n",
    "        json.dump(train_dataset, f, indent=4)\n",
    "\n",
    "    return df, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGN3xV0_D5cn",
    "outputId": "98b39504-989b-44e5-fb03-8768d0fab084"
   },
   "outputs": [],
   "source": [
    "df, train_dataset = get_train_set(original_df, 'gpt-4', balanced=True, reversed=False)\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(train_dataset[0]['input'])\n",
    "print(train_dataset[0]['output'])\n",
    "len(train_dataset), df['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJoM-x6dkd_Z",
    "outputId": "d47c3d0c-3cc0-4c0d-e2d0-cd388e68dc41"
   },
   "outputs": [],
   "source": [
    "df, train_dataset = get_train_set(original_df, 'gpt-4', balanced=True, reversed=True)\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(train_dataset[0]['input'])\n",
    "print(train_dataset[0]['output'])\n",
    "len(train_dataset), df['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entshgjAkmkG",
    "outputId": "36130d33-736e-46d0-efef-5f8db333ff57"
   },
   "outputs": [],
   "source": [
    "df, train_dataset = get_train_set(original_df, 'gpt-4', balanced=False, reversed=False)\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(train_dataset[0]['input'])\n",
    "print(train_dataset[0]['output'])\n",
    "len(train_dataset), df['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vqPbQ1CkuvV",
    "outputId": "b57c39ee-3724-41cc-d9c8-5a75ae3db181"
   },
   "outputs": [],
   "source": [
    "df, train_dataset = get_train_set(original_df, 'gpt-4', balanced=False, reversed=True)\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(train_dataset[0]['input'])\n",
    "print(train_dataset[0]['output'])\n",
    "len(train_dataset), df['ground_truth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG5lTqaHonAU"
   },
   "outputs": [],
   "source": [
    "df, train_dataset = get_train_set(original_df, 'gpt-3_5', balanced=True, reversed=False)\n",
    "df, train_dataset = get_train_set(original_df, 'gpt-3_5', balanced=True, reversed=True)\n",
    "df, train_dataset = get_train_set(original_df, 'gpt-3_5', balanced=False, reversed=False)\n",
    "df, train_dataset = get_train_set(original_df, 'gpt-3_5', balanced=False, reversed=True)\n",
    "\n",
    "df, train_dataset = get_train_set(original_df, '70b', balanced=True, reversed=False)\n",
    "df, train_dataset = get_train_set(original_df, '70b', balanced=True, reversed=True)\n",
    "df, train_dataset = get_train_set(original_df, '70b', balanced=False, reversed=False)\n",
    "df, train_dataset = get_train_set(original_df, '70b', balanced=False, reversed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__ke_uC52ZX"
   },
   "source": [
    "# Make test set json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCslQ1kydAFe"
   },
   "outputs": [],
   "source": [
    "# test (original order)\n",
    "test_dataset = []\n",
    "for i, row in original_df.iterrows():\n",
    "    tweet = row['tweet']\n",
    "    claim = row['claim']\n",
    "    ground_truth = row['Mturk_1']\n",
    "\n",
    "    datum = {\n",
    "\n",
    "\"instruction\": f\"\"\"<<SYS>> Which of the following best describes the relationship between TWEET and CLAIM?\n",
    "\n",
    "If TWEET is true:\n",
    "(ENTAILMENT) then CLAIM is also true.\n",
    "(NEUTRAL) CLAIM cannot be said to be true or false.\n",
    "(CONTRADICTION) then CLAIM is false. <</SYS>>\"\"\",\n",
    "\n",
    "\"input\": f\"\"\"TWEET: {tweet}\n",
    "Claim: {claim}\n",
    "Answer:\"\"\",\n",
    "\n",
    "\"output\": f\"{ground_truth}\"\n",
    "    }\n",
    "\n",
    "    test_dataset.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi9eGdxcdM1I"
   },
   "outputs": [],
   "source": [
    "with open('data/LLaMA-Efficient-Tuning/data/test.json', 'w') as f:\n",
    "    json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-xTsN7Ldd4H"
   },
   "outputs": [],
   "source": [
    "# test (reversed order)\n",
    "test_dataset_r = []\n",
    "for i, row in original_df.iterrows():\n",
    "    tweet = row['tweet']\n",
    "    claim = row['claim']\n",
    "    ground_truth = row['Mturk_1_reversed']\n",
    "\n",
    "    datum = {\n",
    "\n",
    "\"instruction\": f\"\"\"<<SYS>> Which of the following best describes the relationship between CLAIM and TWEET?\n",
    "\n",
    "If CLAIM is true:\n",
    "(ENTAILMENT) then TWEET is also true.\n",
    "(NEUTRAL) TWEET cannot be said to be true or false.\n",
    "(CONTRADICTION) then TWEET is false. <</SYS>>\"\"\",\n",
    "\n",
    "\"input\": f\"\"\"CLAIM: {claim}\n",
    "TWEET: {tweet}\n",
    "ANSWER:\"\"\",\n",
    "\n",
    "\"output\": f\"{ground_truth}\"\n",
    "    }\n",
    "\n",
    "    test_dataset_r.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6S9XPlHdd1X"
   },
   "outputs": [],
   "source": [
    "with open('data/LLaMA-Efficient-Tuning/data/test_r.json', 'w') as f:\n",
    "    json.dump(test_dataset_r, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
