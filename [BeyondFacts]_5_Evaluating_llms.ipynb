{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp-wnw44P6I6"
   },
   "source": [
    "# Open dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5LG-pqBFSpl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igVNpufMbUK2"
   },
   "outputs": [],
   "source": [
    "load_path = 'data/data_final.csv'\n",
    "original_df = pd.read_csv(load_path, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou82w2xFxgLv"
   },
   "outputs": [],
   "source": [
    "columns_eval = ['annotation_only_gpt-4',\n",
    " 'annotation_only_gpt-3_5',\n",
    " 'annotation_only_13b',\n",
    " 'annotation_only_7b',\n",
    " 'gpt-3_5_finetuned_on_gpt_4',\n",
    " 'gpt-3_5_finetuned_on_gpt_3_5',\n",
    " 'gpt-3_5_finetuned_on_70b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoB6Xtzbg7iz"
   },
   "outputs": [],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfzcVk2VwPqO"
   },
   "source": [
    "# Evaluation by Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HigdSgYWFYJE"
   },
   "source": [
    "## Set ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgH7zLW70rGk"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Columns for 'Mturk~' and 'Mturk~_reversed'\n",
    "mturk_columns = ['Mturk_1', 'Mturk_2', 'Mturk_3', 'Mturk_4', 'Mturk_5']\n",
    "mturk_reversed_columns = ['Mturk_1_reversed', 'Mturk_2_reversed', 'Mturk_3_reversed', 'Mturk_4_reversed', 'Mturk_5_reversed']\n",
    "\n",
    "# Function to calculate the majority vote, considering ties\n",
    "def majority_vote(row):\n",
    "    counter = Counter(row)\n",
    "    max_count = max(counter.values())\n",
    "    return [k for k, v in counter.items() if v == max_count]\n",
    "\n",
    "# Calculate the majority vote for 'Mturk~'\n",
    "original_df['Majority_Mturk'] = original_df[mturk_columns].apply(majority_vote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpEJUzBVSFoF"
   },
   "outputs": [],
   "source": [
    "print(647/1225)\n",
    "print(433/1225)\n",
    "print(90/1225)\n",
    "print(55/1225)\n",
    "\n",
    "original_df['Majority_Mturk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQpCOaOHyYwV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Function to randomly tie-break and aggregate based on the given scheme\n",
    "def random_tie_break(val1_list):\n",
    "    val1 = np.random.choice(val1_list, 1)[0] if isinstance(val1_list, list) else val1_list\n",
    "    label_set = {'ENTAILMENT', 'NEUTRAL', 'CONTRADICTION'}\n",
    "\n",
    "    if val1 == 'ENTAILMENT':\n",
    "        return 'ENTAILMENT'\n",
    "    elif val1 == 'CONTRADICTION':\n",
    "        return 'CONTRADICTION'\n",
    "    elif val1 not in label_set:\n",
    "        return None\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert the relevant columns to NumPy arrays for faster computation\n",
    "majority_mturk = original_df['Majority_Mturk'].to_numpy()\n",
    "\n",
    "# Initialize a list to store the randomly tie-broken and aggregated lists\n",
    "random_aggregated_mturks = []\n",
    "\n",
    "# Create 10 random tie-broken and aggregated lists as a demonstration\n",
    "for i in range(1000):\n",
    "    random_aggregated_mturk = [random_tie_break(val1) for val1 in majority_mturk]\n",
    "    random_aggregated_mturks.append(random_aggregated_mturk)\n",
    "\n",
    "# Pickle the random_aggregated_mturks list\n",
    "pickle_file_path = 'data/aggregate.pkl'\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(random_aggregated_mturks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOBzMILGRyrl"
   },
   "outputs": [],
   "source": [
    "# Load the pickled random_aggregated_mturks list\n",
    "pickle_file_path = 'data/aggregate.pkl'\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    random_aggregated_mturks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMsNamoL4GlB"
   },
   "outputs": [],
   "source": [
    "# Initialize a Counter to store the frequency of each class across all numpies\n",
    "overall_counter = Counter()\n",
    "\n",
    "# Count class frequencies across the 1,000 randomly aggregated lists\n",
    "for random_aggregated_mturk in random_aggregated_mturks:\n",
    "    counter = Counter(random_aggregated_mturk)\n",
    "    overall_counter += counter\n",
    "\n",
    "# Calculate the average frequency of each class\n",
    "total_count = sum(overall_counter.values())\n",
    "average_ratio = {k: v / total_count for k, v in overall_counter.items()}\n",
    "average_frequency = {k: v / 1000 for k, v in overall_counter.items()}\n",
    "average_ratio, average_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC4owfnjFsV2"
   },
   "source": [
    "## Preprocess predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmCgWleJQHtM"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_all_annotation(s, front):\n",
    "    s = str(s).upper()\n",
    "    tokens = re.split(r'\\W', s)  # split on non-alphanumeric characters\n",
    "\n",
    "    valid_tokens_map = {\n",
    "        'ENTAIL': 'ENTAILMENT',\n",
    "        'ENTAILS': 'ENTAILMENT',\n",
    "        'ENTAILING': 'ENTAILMENT',\n",
    "        'CONTRADICT': 'CONTRADICTION',\n",
    "        'CONTRADICTS': 'CONTRADICTION',\n",
    "        'CONTRADICTING': 'CONTRADICTION'\n",
    "    }\n",
    "\n",
    "    valid_tokens = set([\n",
    "        'ENTAILMENT', 'NEUTRAL', 'CONTRADICTION',\n",
    "    ])\n",
    "\n",
    "    annotate_list = [valid_tokens_map.get(token, token) for token in tokens if token in valid_tokens or token in valid_tokens_map]\n",
    "\n",
    "    if len(annotate_list) == 0:\n",
    "        return None\n",
    "    elif front:\n",
    "        return annotate_list[0]\n",
    "    else:\n",
    "        return annotate_list[-1]\n",
    "\n",
    "for col in columns_eval:\n",
    "    if col.startswith('zero_shot_cot') or col.startswith('few_shot'):\n",
    "        original_df[col] = original_df[col].apply(lambda x: find_all_annotation(x, front=False))\n",
    "    else:\n",
    "        original_df[col] = original_df[col].apply(lambda x: find_all_annotation(x, front=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV7djPYvh6OE"
   },
   "outputs": [],
   "source": [
    "none_indices = {col: original_df[original_df[col].isna()].index.tolist() for col in columns_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URqEbIkShak7"
   },
   "outputs": [],
   "source": [
    "# Function to count rows that are not one word\n",
    "def count_non_single_word_rows(column):\n",
    "    return sum(original_df[column].apply(lambda x: len(str(x).split()) != 1))\n",
    "\n",
    "# Iterate through columns ending with '_cleaned' and apply the counting function\n",
    "counts = {column: count_non_single_word_rows(column) for column in original_df.columns if column in columns_eval}\n",
    "\n",
    "# Print the results\n",
    "for column, count in counts.items():\n",
    "    print(f\"{column}: {count} rows are not one word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUZdWjyiFdcC"
   },
   "source": [
    "## Evaluate over 1,000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20PlCWEF0vCx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, accuracy_score, f1_score\n",
    "from itertools import product\n",
    "\n",
    "def evaluate_aggregated_mturks(df, column_name, sample=1000):\n",
    "    # Extract the relevant columns\n",
    "    column = df[column_name].to_numpy()\n",
    "\n",
    "    # Initialize lists to store precision, recall, and accuracy\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    mic_precision_list = []\n",
    "    mic_recall_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    # Initialize a counter for None values\n",
    "    none_counter = 0\n",
    "\n",
    "    # Initialize a list to store the indices of None values\n",
    "    none_indices = []\n",
    "\n",
    "    # Generate 1000 random tie-broken and aggregated numpy arrays and evaluate metrics\n",
    "    for i in range(sample):\n",
    "        aggregated_col = []\n",
    "        for idx, val1 in enumerate(column):\n",
    "            if val1 is None:\n",
    "                none_counter += 1\n",
    "                none_indices.append(idx)\n",
    "                aggregated_col.append(None)\n",
    "                continue\n",
    "            aggregated_col.append(random_tie_break(val1))\n",
    "\n",
    "        aggregated_col = np.array(aggregated_col)\n",
    "\n",
    "        # Filter out None values for the metric calculations\n",
    "        valid_indices = aggregated_col != None\n",
    "        filtered_aggregated_col = aggregated_col[valid_indices]\n",
    "        filtered_reference = np.array([random_aggregated_mturk[i] for i in range(len(random_aggregated_mturk)) if valid_indices[i]])\n",
    "\n",
    "        # Calculate and store precision, recall, and accuracy\n",
    "        precision_list.append(precision_score(filtered_reference, filtered_aggregated_col, average='macro', zero_division=0))\n",
    "        recall_list.append(recall_score(filtered_reference, filtered_aggregated_col, average='macro', zero_division=0))\n",
    "        mic_precision_list.append(precision_score(filtered_reference, filtered_aggregated_col, average='micro', zero_division=0))\n",
    "        mic_recall_list.append(recall_score(filtered_reference, filtered_aggregated_col, average='micro', zero_division=0))\n",
    "        accuracy_list.append(accuracy_score(filtered_reference, filtered_aggregated_col))\n",
    "\n",
    "        # Calculate precision for each label\n",
    "        con_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[0]\n",
    "        ent_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[1]\n",
    "        neu_precision_list = precision_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[2]\n",
    "\n",
    "        # Calculate recall for each label\n",
    "        con_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[0]\n",
    "        ent_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[1]\n",
    "        neu_recall_list = recall_score(filtered_reference, filtered_aggregated_col, average=None, zero_division=0)[2]\n",
    "\n",
    "        # Calculate accuracy for each label\n",
    "        con_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[0]\n",
    "        ent_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[1]\n",
    "        neu_f1_list = f1_score(filtered_reference, filtered_aggregated_col, average=None)[2]\n",
    "\n",
    "\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    avg_mic_precision = np.mean(mic_precision_list)\n",
    "    avg_mic_recall = np.mean(mic_recall_list)\n",
    "    avg_accuracy = np.mean(accuracy_list)\n",
    "\n",
    "    ent_precision = np.mean(ent_precision_list)\n",
    "    neu_precision = np.mean(neu_precision_list)\n",
    "    con_precision = np.mean(con_precision_list)\n",
    "\n",
    "    ent_recall = np.mean(ent_recall_list)\n",
    "    neu_recall = np.mean(neu_recall_list)\n",
    "    con_recall = np.mean(con_recall_list)\n",
    "\n",
    "    ent_f1 = np.mean(ent_f1_list)\n",
    "    neu_f1 = np.mean(neu_f1_list)\n",
    "    con_f1 = np.mean(con_f1_list)\n",
    "\n",
    "\n",
    "    print(f\"\"\"Overall performance\n",
    "Macro precision: {avg_precision}\n",
    "Macro recall: {avg_recall}\n",
    "Micro precision: {avg_mic_precision}\n",
    "Micro recall: {avg_mic_recall}\n",
    "Accuracy: {avg_accuracy}\n",
    "\n",
    "Label-by-label (order: E-N-C)\n",
    "Precision: {ent_precision}, {neu_precision}, {con_precision}\n",
    "Recall: {ent_recall}, {neu_recall}, {con_recall}\n",
    "F1-score: {ent_f1}, {neu_f1}, {con_f1}\n",
    "\n",
    "None: {none_counter}\"\"\")\n",
    "\n",
    "    return avg_precision, avg_recall, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi8na42XIM46"
   },
   "outputs": [],
   "source": [
    "# Test the function with 'Majority_Mturk' column and 'gpt-3_5_finetuned_on_gpt_4' as the reference column\n",
    "for col in columns_eval:\n",
    "    try:\n",
    "        print(col)\n",
    "        evaluate_aggregated_mturks(original_df, col, sample=1000)\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0kEOQTVZcPN"
   },
   "source": [
    "# Fine-tune llamas and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC002-NNfmMA"
   },
   "outputs": [],
   "source": [
    "finetuned_model_size_list = ['13b', '7b']\n",
    "trainset_model_list = ['gpt-4', 'gpt-3_5', 'llama_2_70b']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for finetuned_model_size, trainset_model in product(finetuned_model_size_list, trainset_model_list):\n",
    "    model_config_folder = f'train_valid_{finetuned_model_size}_train_{trainset_model}_balanced'\n",
    "\n",
    "    for number in range(61, 550, 61):\n",
    "        file_path = f'data/Finetuning/{model_config_folder}/test-{number}/generated_predictions.jsonl'\n",
    "        df[f'{number}_{model_config_folder}'] = pd.read_json(file_path, lines=True)['predict']\n",
    "\n",
    "# Function to clean cells and standardize to first occurrence of {ENTAILMENT, NEUTRAL, CONTRADICTION}\n",
    "def clean_and_standardize(cell):\n",
    "    standard_labels = {'ENTAILMENT', 'NEUTRAL', 'CONTRADICTION', 'no relationship'}\n",
    "    if isinstance(cell, str):\n",
    "        for label in standard_labels:\n",
    "            if label in cell:\n",
    "                return 'NEUTRAL' if label == 'no relationship' else label\n",
    "    return cell\n",
    "\n",
    "\n",
    "# Apply the function to clean and standardize the DataFrame\n",
    "df = df.applymap(clean_and_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqBuxU80H2k-"
   },
   "outputs": [],
   "source": [
    "for finetuned_model_size, trainset_model in product(finetuned_model_size_list, trainset_model_list):\n",
    "\n",
    "    try:\n",
    "        model_config_folder = f'train_valid_{finetuned_model_size}_train_{trainset_model}_balanced'\n",
    "        col = f'549_{model_config_folder}'\n",
    "        print(col)\n",
    "        print(evaluate_aggregated_mturks(df, col, sample=1000))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxFAVPch1M_Q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def find_all_occurrences_entailment(s):\n",
    "    s = str(s)\n",
    "    tokens = re.split(r'\\W', s)  # split on non-alphanumeric characters\n",
    "    valid_tokens = ['ENTAILMENT', 'NEUTRAL', 'CONTRADICTION']\n",
    "    return ' '.join(list(set([token for token in tokens if token in valid_tokens])))\n",
    "\n",
    "# Function to count rows that are not one word\n",
    "def count_non_single_word_rows(column):\n",
    "    return sum(original_df[column].apply(lambda x: len(str(x).split()) != 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zol3TnlRgB-4"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_13b_train_gpt-4_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_13b_train_gpt-4_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-13b-chat-hf\\nTraining set: GPT-4', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfV_D66ouGxr"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_13b_train_gpt-3_5_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_13b_train_gpt-3_5_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-13b-chat-hf\\nTrain set: GPT-3.5-Turbo', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3b4sm7ob34i"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_13b_train_llama_2_70b_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_13b_train_llama_2_70b_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-13b-chat-hf\\nTrain set: Llama-2-70b-chat-hf', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGHsGdMMcOwS"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_7b_train_gpt-4_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_7b_train_gpt-4_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-7b-chat-hf\\nTrain set: GPT-4', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvyJzFk9cm_v"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_7b_train_gpt-3_5_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_7b_train_gpt-3_5_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-7b-chat-hf\\nTrain set: GPT-3.5 Turbo', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flc11U81czAX"
   },
   "outputs": [],
   "source": [
    "steps = [61, 122, 183, 244, 305, 366, 427, 488, 549]\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "for number in range(61, 550, 61):\n",
    "    P, R, A = evaluate_aggregated_mturks(df, f\"{number}_train_valid_7b_train_llama_2_70b_balanced\", sample=1000)\n",
    "    print(f\"{number}_train_valid_7b_train_llama_2_70b_balanced\")\n",
    "    precision.append(P)\n",
    "    recall.append(R)\n",
    "    accuracy.append(A)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_and_process_log(file_path):\n",
    "    log_entries = []\n",
    "    train_loss_by_epoch = {}\n",
    "    validation_loss_by_epoch = {}\n",
    "\n",
    "    # Read the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entries.append(json.loads(line.strip()))\n",
    "\n",
    "    # Populate the lists by epoch\n",
    "    for entry in log_entries:\n",
    "        epoch = entry['epoch']\n",
    "        if entry['loss'] is not None:\n",
    "            train_loss_by_epoch.setdefault(epoch, []).append(entry['loss'])\n",
    "        if entry['eval_loss'] is not None:\n",
    "            validation_loss_by_epoch.setdefault(epoch, []).append(entry['eval_loss'])\n",
    "\n",
    "    # Average the losses for each epoch\n",
    "    avg_train_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in train_loss_by_epoch.items()}\n",
    "    avg_validation_loss_by_epoch = {epoch: np.mean(losses) for epoch, losses in validation_loss_by_epoch.items()}\n",
    "\n",
    "    # Sort by epoch for plotting\n",
    "    sorted_train_epochs = sorted(avg_train_loss_by_epoch.keys())\n",
    "    sorted_train_loss = [avg_train_loss_by_epoch[epoch] for epoch in sorted_train_epochs]\n",
    "\n",
    "    sorted_validation_epochs = sorted(avg_validation_loss_by_epoch.keys())\n",
    "    sorted_validation_loss = [avg_validation_loss_by_epoch[epoch] for epoch in sorted_validation_epochs]\n",
    "\n",
    "    return sorted_train_epochs, sorted_train_loss, sorted_validation_epochs, sorted_validation_loss\n",
    "\n",
    "# Read and process the logs for 'tweet-claim' and 'claim-tweet' presentation orders\n",
    "tweet_claim_epochs, tweet_claim_train_loss, tweet_claim_val_epochs, tweet_claim_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced/trainer_log.jsonl')\n",
    "claim_tweet_epochs, claim_tweet_train_loss, claim_tweet_val_epochs, claim_tweet_val_loss = read_and_process_log('data/Finetuning/train_valid_13b_train_gpt-4_balanced_reversed/trainer_log.jsonl')\n",
    "\n",
    "# Function to smooth data using a simple moving average\n",
    "def smooth_data(data, window_size=5):\n",
    "    smoothed_data = []\n",
    "    for i in range(len(data)):\n",
    "        start_idx = max(0, i - window_size // 2)\n",
    "        end_idx = min(len(data), i + window_size // 2 + 1)\n",
    "        window_data = data[start_idx:end_idx]\n",
    "        smoothed_data.append(np.mean(window_data))\n",
    "    return smoothed_data\n",
    "\n",
    "# Smooth the training curves\n",
    "smoothed_tweet_claim_train_loss = smooth_data(tweet_claim_train_loss)\n",
    "smoothed_claim_tweet_train_loss = smooth_data(claim_tweet_train_loss)\n",
    "\n",
    "# Calculate the corresponding epochs for each step\n",
    "epochs_for_metrics = np.array(steps) / 183\n",
    "\n",
    "# Create the plot with specified figsize, bigger fonts and thicker lines\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Plot loss curves with thicker lines\n",
    "ax1.plot(tweet_claim_epochs, smoothed_tweet_claim_train_loss, label='Train Loss (Tweet-Claim)', linewidth=5, color='teal')\n",
    "ax1.plot(tweet_claim_val_epochs, tweet_claim_val_loss, label='Validation Loss', linewidth=5, color='orange')\n",
    "ax1.plot(claim_tweet_epochs, smoothed_claim_tweet_train_loss, label='Train Loss (Claim-Tweet)', linewidth=5, color='dodgerblue')\n",
    "ax1.plot(claim_tweet_val_epochs, claim_tweet_val_loss, label='Validation Loss', linewidth=5, color='red')\n",
    "\n",
    "# Labels, title and grid for ax1\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Average Loss')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Base model: Llama-2-7b-chat-hf\\nTrain set: Llama-2-70b-chat-hf', fontsize=30, loc='left')\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "# Create another y-axis for the metrics\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 1])  # Limit y-axis to [0, 1]\n",
    "ax2.set_ylabel('Performance Metrics')\n",
    "# ax2.grid(axis='y', linestyle='--', linewidth=0.7, color='gray')\n",
    "for y in np.linspace(0, 1, num=5):  # Replace with your actual y-values\n",
    "    ax2.axhline(y, color='gray', linestyle='--', linewidth=0.7)\n",
    "\n",
    "ax1.set_yticks([0.25, 0.5, 0.75])\n",
    "ax1.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "ax2.set_yticks([0.25, 0.5, 0.75])\n",
    "ax2.set_yticklabels(['0.25', '0.5', '0.75'])\n",
    "\n",
    "# Plot the metrics on the new y-axis with thicker lines\n",
    "ax2.plot(epochs_for_metrics, precision, label='Precision', marker='o', linestyle='--', linewidth=5, color='magenta')\n",
    "ax2.plot(epochs_for_metrics, recall, label='Recall', marker='s', linestyle='--', linewidth=5, color='green')\n",
    "ax2.plot(epochs_for_metrics, accuracy, label='Accuracy', marker='x', linestyle='--', linewidth=5, color='purple')\n",
    "\n",
    "\n",
    "# Add divided legend with solid background\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1, labels1, loc='upper left', framealpha=1.0)\n",
    "ax2.legend(lines2, labels2, loc='upper right', framealpha=1.0)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TH0ZoDYOwMq3",
    "eC4owfnjFsV2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
